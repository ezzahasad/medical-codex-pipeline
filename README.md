hello 
As a beginner, the provided template given by the professor, google searches, AI assistance (including help directly through VS code), and our module lectures from class were very beneficial for working through the assignment and guiding me. I was able to start with a base, further experiment with scripts, and adjust my approaches and data that allowed me to get my end results. The following sections below briefly discuss what I did for each codex, and highlights some issues/mistakes I had in which I tried to resolve. 

Hcps_processor.py
For the HCPCS dataset, I started with a raw .txt file containing many irregular columns, and made it messier than expected. At first, I assumed there were only two fields (code and description) which caused repeated KeyError and ParserError messages when Pandas tried to interpret the file. In order to fix this issue, I checked the first few rows and noticed that the structure included extra flags, numeric values, and inconsistent spacing. I rewrote the script (hcps_processor.py) to load the file with flexible whitespace parsing by writing  pd.read_csv("input/HCPC2025_OCT_ANWEB_v3.txt", delim_whitespace=True, header=None, dtype=str). Once loaded, I renamed only the first two fields that were mainly necessary, using hcpcs.rename(columns={0: "code", 1: "description"})[["code", "description"]], and chained .dropna() to remove any empty rows. This validated that only the code and description columns were preserved, while all the extra unnamed fields were removed. The last_updated column was added,  removed duplicates, and exported the results to output/csv/hcps_clean.csv.  To confirm the cleanup, I checked the shape and compared row counts before and after, showing that the dataset went from about 16,000 noisy rows down to 8,800 valid entries with the correct three-column structure (code, description, last_updated). 

Icd10cm_processor.py
For the ICD-10-CM dataset, I built a processor script  (icd10cm_processor.py) that used pandas to load the raw icd102019syst_codes.txt file in order to convert it into a clean CSV. Initially, the output was only showing placeholder values like T,X. As this was the first processor script I was testing, I asked an AI helper tool to explain the issue and stated that the script was pulling the wrong columns (iloc[:, [1, 2]].  I put debug statements that allowed me to see that the actual codes and description were stored in the “A00” and “Cholera” columns in the raw file. This allowed me to know to apply a cleaner function where it would use these columns instead. From this fix, the script was able to produce the correct ICD-10 codes with the descriptions. I then updated the cleaner for it to add the last_updated column, and this added the timestamp for each row. To clean up some more, I also tested removing the duplicates with drop_duplicates(). Initially, I thought I was successful as it did trim down the dataset, but then noticed it was reduced to 264 rows in comparison to 12,220, which felt too drastic of a difference to the raw data. Thus, I reverted this step and kept the duplicates in the script while saving all rows in the cleaned output. While the step was undone, I learned how to use the function and learned to reason based on the outputs given. 

Icd10who_processor.py
For the ICD-10 WHO dataset (icd102019syst_codes.txt), I created the icd10who_processor.py to clean and standardize the codes. The script reads in the raw WHO file through pandas, keeps only the code and description columns, and adds a last_updated field for consistency across codexes. I ran the processor through python3 -m scripts.icd10who_processor, which ensured that the pipeline was end-to-end working and wrote out the result to output/csv/icd10who_clean.csv. Initially, I tried to script to remove the duplicates, but it reduced the data set from 12,220 to 263 which was too low for a data set of this size. Instead, I decided to empty rows by adding simple checks in the script to print row counts before and after cleanup such as print("Rows before cleanup:", len(raw_data)) before dropping and print("Rows after cleanup:", len(clean_data)). The terminal output showed- Rows before cleanup: 12220, Rows after cleanup: 12220, validating that there were no empty codes or descriptions to remove and that the dataset was complete. 

Loinc_processor.py
For the LOINC dataset, we first created a minimal version of the processor script (loinc_processor.py) just to ensure the pipeline worked before doing full cleaning. This test script loaded the raw Loinc.csv file from the input/ folder with pandas, selected only the first five rows so it would run quickly, and then saved those rows into the output/csv/ folder as loinc_clean.csv using the save_to_formats helper from common_functions.py. This step allowed me to know that the script could successfully read the data, the helper function ran accurately, and files were being saved to the correct folder. I also ran into a very minor import error (but still wanted to note) when I tried to call save_to_csv instead of save_to_formats, which taught me to carefully match the function name in my script with what was actually defined in common_functions.py. After confirming the basic pipeline, I expanded the script to perform more realistic cleanup by keeping the LOINC_NUM and LONG_COMMON_NAME columns, renaming them to code and description, dropping missing rows, trimming whitespace, and adding a last_updated timestamp. This created a cleaned file saved as output/csv/loinc_clean.csv. I also attempted duplicate validation, but wasn’t able to fully complete that step as I kept getting an error which I noted in my commit. 


Npi_processor.py
For the NPI_processor.py data set, the processor script was built and I used pandas in order to load the raw file, highlighting to extract the necessary fields which were the NPI number and the provider first and last name. I also dropped rows with missing values, and added a last_updated column that had today’s date. Since the data set was very large, it was important to have the script saved as two outputs since the clean file would be too large to push to github (a full cleaned file (output/csv/npi_clean.csv) and a smaller 100-row sample (output/csv/npi_sample.csv). To create these two separate files, I initially ran the command python3 -m scripts.npi_processor which worked; however, it still caused an error when pushing it to Github as it was over the limit. I attempted to update .gitignore as well as using git rm–cached, but there were errors since the file was somewhere stuck within the repository history. I forced a clean push with git filter-repo --path output/csv/npi_clean.csv --invert-paths
git push origin main --force, which allowed the smaller npi_sample.csv in place but removed the large file from any history. In .gitignore, I added output/csv/npi_clean.csv so that there is no tracking and will only have the lightweight sample file for review in the repository, but still have access to the full data set as well. 

Rxnorm_processor.py
For the RxNorm processor, I started by creating a script to read the raw .RRF input file. I chose Polars for this dataset because it handles large files well and allows me to try a different kind of script outside of pandas. Initially, I faced confusion with column indexing in Polars and encountered errors about missing columns, which took some trial and error to resolve. Once I corrected the selections, I extracted the necessary fields (code and description) and added a last_updated column for consistency with the other codex outputs. Another issue was that the script did not automatically create the output/csv folder, leading to errors when saving the cleaned file. After I manually created the folder and re-ran the script, the cleaned output was saved successfully. I also dealt with some Git-related problems where pushes failed at first, so I committed the script early to show progress before completing the cleanup. In order to validate results, I loaded the cleaned file into Pandas with a short snippet: import pandas as pd df = pd.read_csv("output/csv/rxnorm_clean.csv") print(df.shape) print(df.head()). This showed that the cleaned file contained only three columns (code, description, last_updated) and no empty rows.

Snomed_processor.py
For the SNOMED dataset, I used snomed_processor.py to process the large raw description file (sct2_Description_Full-en_US1000124_20250901.txt). I loaded the file with pandas and kept only the id and term columns, renaming them to code and description. To initiate the cleanup,  I dropped any rows missing values in those fields, added a last_updated column to record when the file was processed, and included row counts before and after cleanup to confirm the results. The terminal reported that the rows before cleanup were 2,918,822, and rows after cleanup were 2,918,817, showing that only a few rows were removed and that the difference was minor. An error I had made was trying to commit the SNOMED file which had almost 3 million rows, so the push kept failing. While I tried to get assistance and play with different scripts, the resolution was to run git reset HEAD~3, which removes the last three commits, including the large file from the repo’s history. I also added a .gitignore rule so the large snomed_clean.csv would be ignored in the future. After that, I staged and committed only a smaller snomed_sample.csv, which includes the first 100 rows. By doing this, the file can be pushed to GitHub without issues while still showing the cleaning process. This way, the fully cleaned dataset stays available locally for analysis, while the repository remains lightweight and meets GitHub’s file limits. 